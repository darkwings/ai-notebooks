{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs5hLs2dhpNr0U/Epg5n4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkwings/ai-notebooks/blob/main/ReAct_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReAct agents\n",
        "\n",
        "A simple ReAct agent with LangGraph\n",
        "\n"
      ],
      "metadata": {
        "id": "8BgBR0OXT3VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_openai langchain_core langchain_community langgraph"
      ],
      "metadata": {
        "id": "j3LuvqRRT_fw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfd20kMAUCOX",
        "outputId": "049eb08c-9334-443c-cebb-922cad492a57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import all the necessary stuff"
      ],
      "metadata": {
        "id": "WvZ2c2VNUQds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated, List\n",
        "import operator\n",
        "from langchain_core.messages import (\n",
        "    AnyMessage,\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    ToolMessage,\n",
        "    AIMessage\n",
        ")\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "HvpshUqNUS8c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can implement the needed classes.\n",
        "\n",
        "Let's start with the prompt"
      ],
      "metadata": {
        "id": "FFc8vRgnUZKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a more structured prompt template with tool descriptions\n",
        "prompt_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "Question: {input}\n",
        "Thought: {agent_scratchpad}\"\"\"\n"
      ],
      "metadata": {
        "id": "n8L4vSAcUcnr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the State and the tool. We simulate a tool that returns the rating of some restaurants"
      ],
      "metadata": {
        "id": "hcA2p00dUvzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[AnyMessage], operator.add]\n",
        "\n",
        "# This is the tool\n",
        "# The tool could be implemented also as a function, in this case\n",
        "# we use a class. LangGraph will call the __call__ function\n",
        "class RestaurantTool:\n",
        "    def __init__(self):\n",
        "        self.name = \"restaurant_rating\"\n",
        "        self.description = \"Get rating and review information for a restaurant\"\n",
        "\n",
        "    def get_restaurant_rating(self, name: str) -> dict:\n",
        "        ratings = {\n",
        "            \"Pizza Palace\": {\"rating\": 4.5, \"reviews\": 230},\n",
        "            \"Burger Barn\": {\"rating\": 4.2, \"reviews\": 185},\n",
        "            \"Sushi Supreme\": {\"rating\": 4.8, \"reviews\": 320}\n",
        "        }\n",
        "        return ratings.get(name, {\"rating\": 0, \"reviews\": 0})\n",
        "\n",
        "    def __call__(self, name: str) -> str:\n",
        "        result = self.get_restaurant_rating(name)\n",
        "        return f\"Rating: {result['rating']}/5.0 from {result['reviews']} reviews\""
      ],
      "metadata": {
        "id": "aNUlOfn-U4NH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent class\n",
        "\n",
        "Now the agent class"
      ],
      "metadata": {
        "id": "PdCicKpWU7PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReActAgent:\n",
        "    def __init__(self, model: ChatOpenAI, tools: List[Tool], system: str = ''):\n",
        "        self.system = system\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "\n",
        "        # Create tool descriptions for the prompt\n",
        "        tool_descriptions = \"\\n\".join(f\"- {t.name}: {t.description}\" for t in tools)\n",
        "        tool_names = \", \".join(t.name for t in tools)\n",
        "\n",
        "        # Bind tools to the model\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "        # Initialize the graph\n",
        "        graph = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes and edges\n",
        "        graph.add_node(\"llm\", self.call_llm)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "\n",
        "        # Add conditional edges\n",
        "        graph.add_conditional_edges(\n",
        "            \"llm\",\n",
        "            self.should_continue,\n",
        "            {True: \"action\", False: END}\n",
        "        )\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "\n",
        "        # Set entry point and compile\n",
        "        graph.set_entry_point(\"llm\")\n",
        "        self.graph = graph.compile()\n",
        "\n",
        "    def should_continue(self, state: AgentState) -> bool:\n",
        "        \"\"\"Check if there are any tool calls to process\"\"\"\n",
        "        # The key here is to verify if there are planned tool calls from llm,\n",
        "        # so we take the last message and check if the LLM required tool calls\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        return hasattr(last_message, \"tool_calls\") and bool(last_message.tool_calls)\n",
        "\n",
        "    def call_llm(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Process messages through the LLM\"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        if self.system and not any(isinstance(m, SystemMessage) for m in messages):\n",
        "            messages = [SystemMessage(content=self.system)] + messages\n",
        "        response = self.model.invoke(messages)\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    def take_action(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"Execute tool calls and return results\"\"\"\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        results = []\n",
        "\n",
        "        # Notice that the message include tool calls required by LLM\n",
        "        # in the property tool_calls. We iterate the property and invoke the\n",
        "        # tool.\n",
        "        # The tool call could be simplified using a ToolNode, but this\n",
        "        # code explains clearly what happens under the hood\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            tool_name = tool_call['name']\n",
        "            if tool_name not in self.tools:\n",
        "                result = f\"Error: Unknown tool '{tool_name}'\"\n",
        "            else:\n",
        "                try:\n",
        "                    tool_result = self.tools[tool_name].invoke(tool_call['args'])\n",
        "                    result = str(tool_result)\n",
        "                except Exception as e:\n",
        "                    result = f\"Error executing {tool_name}: {str(e)}\"\n",
        "\n",
        "            results.append(\n",
        "                ToolMessage(\n",
        "                    tool_call_id=tool_call['id'],\n",
        "                    name=tool_name,\n",
        "                    content=result\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return {\"messages\": results}\n",
        "\n",
        "    def invoke(self, message: str) -> List[AnyMessage]:\n",
        "        \"\"\"Main entry point for the agent\"\"\"\n",
        "        initial_state = {\"messages\": [HumanMessage(content=message)]}\n",
        "        final_state = self.graph.invoke(initial_state)\n",
        "        return final_state[\"messages\"]"
      ],
      "metadata": {
        "id": "YgIfLjXPU-hP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent creation\n",
        "\n",
        "Now, we create a function that creates the agent and the tools"
      ],
      "metadata": {
        "id": "rVJDt9CNVJV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_restaurant_agent() -> ReActAgent:\n",
        "    model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "    # Create tool instance\n",
        "    restaurant_tool = RestaurantTool()\n",
        "\n",
        "    # Convert to LangChain Tool\n",
        "    tool = Tool(\n",
        "        name=restaurant_tool.name,\n",
        "        description=restaurant_tool.description,\n",
        "        func=restaurant_tool\n",
        "    )\n",
        "\n",
        "    # Create system prompt\n",
        "    system_prompt = prompt_template.format(\n",
        "        tools=tool.description,\n",
        "        tool_names=tool.name,\n",
        "        input=\"{input}\",\n",
        "        agent_scratchpad=\"{agent_scratchpad}\"\n",
        "    )\n",
        "\n",
        "    # Create and return agent\n",
        "    return ReActAgent(model, [tool], system=system_prompt)"
      ],
      "metadata": {
        "id": "WfXSEEr0VU_s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution\n",
        "\n",
        "Now we execute our agent"
      ],
      "metadata": {
        "id": "dlXck02JVdgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_restaurant_agent()\n",
        "response = agent.invoke(\"\"\"which resturant have better rating, Pizza Palace or Burger Barn?\"\"\")\n",
        "for message in response:\n",
        "    print(f\"{message.type}: {message.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qomv_7zsVsiq",
        "outputId": "ecc7ce49-a295-46b2-ebfb-fbdd722b7af2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human: which resturant have better rating, Pizza Palace or Burger Barn?\n",
            "ai: \n",
            "tool: Rating: 4.5/5.0 from 230 reviews\n",
            "tool: Rating: 4.2/5.0 from 185 reviews\n",
            "ai: Final Answer: Pizza Palace has a better rating with 4.5 out of 5.0 compared to Burger Barn's rating of 4.2 out of 5.0.\n"
          ]
        }
      ]
    }
  ]
}